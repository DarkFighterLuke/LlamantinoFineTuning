{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2732a04b",
   "metadata": {},
   "source": [
    "# Llamantino fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ed7d7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7333d3-8859-4573-bb97-29d0793f126c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/luca/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Set token in HF_TOKEN environment variable before executing\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "576490a3-b497-449f-a164-828cd1b65040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94256a71-be62-44a6-965e-db74dca25b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained model name\n",
    "model_name = \"swap-uniba/LLaMAntino-2-chat-7b-hf-UltraChat-ITA\"\n",
    "\n",
    "# dataset name\n",
    "train_set_name = \"sample_data/train_isear_it.csv\"\n",
    "eval_set_name = \"sample_data/val_isear_it.csv\"\n",
    "test_set_name = \"sample_data/test_isear_it.csv\"\n",
    "\n",
    "# 0 gioia\n",
    "# 1 tristezza\n",
    "# 2 rabbia\n",
    "# 3 paura\n",
    "# 4 vergogna\n",
    "# 5 disgusto\n",
    "# 6 colpevolezza\n",
    "sentiments = [\"gioia\", \"tristezza\", \"rabbia\", \"paura\", \"vergogna\", \"disgusto\", \"colpevolezza\"]\n",
    "\n",
    "# fine-tuned model name\n",
    "new_model = \"llama-2-7b-emotions-final\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04eb9f2",
   "metadata": {},
   "source": [
    "## Dataset splits loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703e88d7-e45b-4c3d-8c9b-2ca9671eb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "train_set = load_dataset(\"csv\", data_files=train_set_name, split=\"train\", delimiter=\"|\")\n",
    "eval_set = load_dataset(\"csv\", data_files=eval_set_name, split=\"train\", delimiter=\"|\")\n",
    "test_set = load_dataset(\"csv\", data_files=test_set_name, split=\"train\", delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ba4dd7-cd39-4118-a0db-3109022aaac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Quando gli americani hanno piazzato i loro missili nucleari in Europa.',\n",
       " 'label': 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8530a80a-e26d-4415-8efe-c303fb3f374b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Ho superato il primo semestre di università.', 'label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c41d84-dc9d-486f-a443-055bc03eedc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Da bambino - essere mandato al negozio dalla mamma - comprare del cioccolato - essere ripreso dalla mamma.',\n",
       " 'label': 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1062ea36",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f8571",
   "metadata": {},
   "source": [
    "Llamantino, as Llama 2 does, needs the prompts to be formatted in a specific way:\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Hi, how are you? [/INST] Good thanks!</s>\n",
    "```\n",
    "\n",
    "Here are the rules:\n",
    "- A single prompt is composed of one or more sequences\n",
    "- A sequence is a pair (user's input, model's output) and its start and its end are indicated with the tags `<s>` and `</s>`\n",
    "- More sequences can be concatenated with `<s>...</s><s>...</s>`\n",
    "- User's input is represented with the tag `[INST]` and `[/INST]`. Each input includes a system prompt and the user's request\n",
    "- `<<SYS>>` and `<</SYS>>` are used to delimit the system prompt, i.e. the way to guide the model interpretation and the generation of the answers to user requests\n",
    "- User's request is appended after the system prompt and after a new line space \\n, without a particular tag\n",
    "- Model's output is appended after the `[/INST]` tag and it ends with `</s>` tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7e6cf41-ef88-4774-9992-4eadfe2ddd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(example):\n",
    "    instruction = \"Sei un assistente disponibile, rispettoso e onesto. \" \\\n",
    "         \"Riconosci quale sentimento esprime la frase che ti viene posta\"\n",
    "    example[\"formatted_instruction\"] = \"\"\"<s>[INST] <<SYS>>\n",
    "{}\n",
    "<</SYS>>\n",
    "\n",
    "{} [/INST] {}</s>\"\"\".format(instruction, example[\"text\"], sentiments[example[\"label\"]])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a7a8c78-82bc-4f8b-aed1-63291cadb1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction_test(example):\n",
    "    instruction = \"Sei un assistente disponibile, rispettoso e onesto. \" \\\n",
    "         \"Riconosci quale sentimento esprime la frase che ti viene posta\"\n",
    "    example[\"formatted_instruction\"] = \"\"\"<s>[INST] <<SYS>>\n",
    "{}\n",
    "<</SYS>>\n",
    "\n",
    "{} [/INST]</s>\"\"\".format(instruction, example[\"text\"])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2d99f58-82c3-4e79-b653-dc319d601f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.map(format_instruction)\n",
    "eval_set = eval_set.map(format_instruction)\n",
    "test_set = test_set.map(format_instruction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a5b400-8979-489a-96b7-c79ee5a526f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Quando gli americani hanno piazzato i loro missili nucleari in Europa.',\n",
       " 'label': 5,\n",
       " 'formatted_instruction': '<s>[INST] <<SYS>>\\nSei un assistente disponibile, rispettoso e onesto. Riconosci quale sentimento esprime la frase che ti viene posta\\n<</SYS>>\\n\\nQuando gli americani hanno piazzato i loro missili nucleari in Europa. [/INST] disgusto</s>'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acf031d2-68d2-4aee-8468-e358daf1483c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Ho superato il primo semestre di università.',\n",
       " 'label': 0,\n",
       " 'formatted_instruction': '<s>[INST] <<SYS>>\\nSei un assistente disponibile, rispettoso e onesto. Riconosci quale sentimento esprime la frase che ti viene posta\\n<</SYS>>\\n\\nHo superato il primo semestre di università. [/INST] gioia</s>'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4604862-5ac6-4b39-9eeb-75e7458f0a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Da bambino - essere mandato al negozio dalla mamma - comprare del cioccolato - essere ripreso dalla mamma.',\n",
       " 'label': 4,\n",
       " 'formatted_instruction': '<s>[INST] <<SYS>>\\nSei un assistente disponibile, rispettoso e onesto. Riconosci quale sentimento esprime la frase che ti viene posta\\n<</SYS>>\\n\\nDa bambino - essere mandato al negozio dalla mamma - comprare del cioccolato - essere ripreso dalla mamma. [/INST]</s>'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b26aa1",
   "metadata": {},
   "source": [
    "### Configure settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b648cc20",
   "metadata": {},
   "source": [
    "Since LLMs have billions of parameters, a technique of parameter efficient tuning (PEFT) is needed, in particular, in order to execute the fine tuning on the available hardware, it is necessary to load the model quantized in 4 bit using QLoRa, that actually represents each parameter using 4 bits only, allowing a huge gain in terms of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b22913c8-3ebb-4887-b397-369c5ab53ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5d90c90d234697a344098bd37dbaf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2633bf1a-f402-4fce-9e2a-86b63d883e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.add_eos_token = False\n",
    "tokenizer.add_bos_token = False\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e09e7fd9-264b-4e3c-9557-2304210557c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    # scale the adapter of lora_alpha/r\n",
    "    lora_alpha= 8,\n",
    "    lora_dropout= 0.1,\n",
    "    r= 16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc5933d5-4f99-43d3-ab2d-6931eec3fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "806abe06-40bd-4986-8a85-d7ab219b747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    num_train_epochs= 1,\n",
    "    per_device_train_batch_size= 4,\n",
    "    gradient_accumulation_steps= 2,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    save_steps= 100,\n",
    "    logging_steps= 20,\n",
    "    learning_rate= 2e-4,\n",
    "    weight_decay= 0.001,\n",
    "    fp16= False,\n",
    "    bf16= False,\n",
    "    max_grad_norm= 0.3,\n",
    "    max_steps= -1,\n",
    "    warmup_ratio= 0.03,\n",
    "    group_by_length= True,\n",
    "    lr_scheduler_type= \"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9381ddad-6fc8-4d49-8f8a-16c2593529be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=eval_set,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= None,\n",
    "    dataset_text_field=\"formatted_instruction\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed3d07",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a98ed61f-0617-4175-a443-467511b1c477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='657' max='657' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [657/657 2:43:39, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>0.833545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.489700</td>\n",
       "      <td>0.823417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.497100</td>\n",
       "      <td>0.813513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.503300</td>\n",
       "      <td>0.808920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.472300</td>\n",
       "      <td>0.805948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.448400</td>\n",
       "      <td>0.804265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for training: 2:44:01.724009\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "start = datetime.now()\n",
    "trainer.train()\n",
    "end = datetime.now()\n",
    "print(f\"Elapsed time for training: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d3c18a1-917c-40ed-84d1-646785072c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(new_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
